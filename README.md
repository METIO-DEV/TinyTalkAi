# Tinyâ€¯Talkâ€¯AI

*Miniâ€‘ChatGPT autoâ€‘hÃ©bergÃ© powered by [Ollama](https://ollama.com) + Laravel + Livewire*

> **Quick pitchÂ :** Tinyâ€¯Talkâ€¯AI fournit une interface de chat ultraâ€‘simple pour interroger des modÃ¨les LLM (LlamaÂ 3, Mistralâ€‘7Bâ€¦). Toute la stack est conteneurisÃ©e et tient sur une machine <Â 2Â vCPUÂ /Â 8Â Go RAM.

---

## âœ¨ FonctionnalitÃ©s clÃ©s

* **UI minimaliste**Â : une zone de dialogue + darkÂ / light mode.
* **ModÃ¨les interchangeables**Â : configuration dans `.env` (ex. `llama3`, `mistral:7b`).
* **Selfâ€‘host**Â : Ollama tourne localement â†’ aucune fuite de donnÃ©es.
* **Authentification Breeze**Â : emailÂ /Â password, protection CSRF.
* **Historique persistant chiffrÃ©** (Ã  venir).
* **CI/CD GitHub Actions**Â : lintÂ + testsÂ + build Docker.

---

## ðŸ—ï¸ Architecture

```
Browser â†’ Laravel Livewire (app) â†’ Ollama daemon
             â†˜                â†˜
              â†˜                â†’ MySQL (data)
               â†’ Redis (queues/cache)
```

* **Sail (Docker Compose)** lance `laravel.test`, `mysql`, `redis`.
* **Ollama** Ã©coute sur `localhost:11434` (ou conteneur `ollama:` selon lâ€™OS).

---

## ðŸš€ Mise en route rapides

### 1. Cloner & prÃ©parer lâ€™app

```bash
git clone git@github.com:Metio-DEV/TinyTalkAI.git
cd TinyTalkAI
cp .env.example .env
composer install
npm install && npm run build
```

### 2. PrÃ©parer Ollama (modÃ¨les, service)

```bash
bash scripts/setup_ollama.sh     # macOS, Linux, WSLÂ 2
```

### 3. DÃ©marrer la stack

```bash
./vendor/bin/sail up -d          # PHP, Nginx, MySQL, Redis
./vendor/bin/sail artisan migrate
open http://localhost            # Page dâ€™accueil Breeze
```

---

## âš™ï¸ Tableau dâ€™installation Ollama

| OS                        | Commandes                                                                       | Remarques                                               |
| ------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------- |
| **macOS**                 | `brew install ollama`<br>`./scripts/setup_ollama.sh`                            | Service `launchd` autoâ€‘start                            |
| **Linux**                 | `curl -fsSL https://ollama.com/install.sh \| sh`<br>`./scripts/setup_ollama.sh` | systemd `ollama.service`                                |
| **WindowsÂ 11/10 + WSLÂ 2** | Ouvrir Ubuntu WSLÂ â†’ `bash scripts/setup_ollama.sh`                              | WSL dÃ©tectÃ© comme Linux                                 |
| **Windows (sans WSL)**    | `docker run -d -p 11434:11434 -v ollama-data:/root/.ollama ollama/ollama`       | Configure `.envÂ OLLAMA_BASE_URL=http://localhost:11434` |

---

## ðŸ“‚ Arborescence importante

```
TinyTalkAI/
â”œâ”€â”€ app/                # Code Laravel
â”œâ”€â”€ public/             # Front assets compilÃ©s (Vite)
â”œâ”€â”€ scripts/            # utilitaires (setup_ollama.sh â€¦)
â”œâ”€â”€ docker-compose.yml  # Sail services
â””â”€â”€ .github/workflows/  # CI (lint, tests, build)
```

---

## ðŸ’¡ Scripts utiles

| Commande                                         | Action                            |
| ------------------------------------------------ | --------------------------------- |
| `./vendor/bin/sail up -d`                        | DÃ©marrer la stack en arriÃ¨reâ€‘plan |
| `./vendor/bin/sail artisan migrate:fresh --seed` | Reset DB de dev                   |
| `./vendor/bin/sail artisan queue:work`           | Traiter les jobs Laravel          |
| `npm run dev`                                    | Vite en mode watch                |

---

## ðŸ§ª Tests

```bash
./vendor/bin/sail test      # exÃ©cute Pest en parallÃ¨le
```

---

## ðŸ“œ Licence

MIT â€“ Â© Metio 2025
